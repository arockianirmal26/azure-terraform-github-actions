{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<mark>Raw-Bronze â†’ Raw ingested data (no/minimal transformations)</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Synapsedemo",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-09-13T08:37:50.3763698Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-13T08:37:50.3781469Z",
              "execution_finish_time": "2025-09-13T08:37:50.6472652Z",
              "parent_msg_id": "6d29ae2c-b6df-4dba-ba2e-35201c82a1f8"
            },
            "text/plain": "StatementMeta(Synapsedemo, 0, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'raw-bronze': 'abfss://raw-bronze@synapsetest1298.dfs.core.windows.net/',\n 'refined-silver': 'abfss://refined-silver@synapsetest1298.dfs.core.windows.net/',\n 'business-gold': 'abfss://business-gold@synapsetest1298.dfs.core.windows.net/'}"
          },
          "execution_count": 25,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Mount ADLS Gen2\n",
        "# Required each time the cluster is restarted which should be only on the first notebook as they run in order\n",
        "\n",
        "# This builds a dictionary mapping each tier to its ADLS Gen2 path\n",
        "tiers = [\"raw-bronze\", \"refined-silver\", \"business-gold\"]\n",
        "adls_paths = {tier: f\"abfss://{tier}@synapsetest1298.dfs.core.windows.net/\" for tier in tiers}\n",
        "\n",
        "# Accessing paths\n",
        "bronze_adls = adls_paths[\"raw-bronze\"]\n",
        "silver_adls = adls_paths[\"refined-silver\"]\n",
        "gold_adls = adls_paths[\"business-gold\"] \n",
        "\n",
        "# printing the paths\n",
        "adls_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Synapsedemo",
              "statement_id": 20,
              "statement_ids": [
                20
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-09-13T08:48:48.0387369Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-13T08:48:48.0404528Z",
              "execution_finish_time": "2025-09-13T08:48:48.3476308Z",
              "parent_msg_id": "e5bc420e-bf48-494e-8f68-3dd1f84fe7e1"
            },
            "text/plain": "StatementMeta(Synapsedemo, 0, 20, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# importing required libraries\n",
        "import requests\n",
        "import json\n",
        "from datetime import date, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Synapsedemo",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-09-13T08:37:57.3059567Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-13T08:37:57.3076737Z",
              "execution_finish_time": "2025-09-13T08:37:57.566685Z",
              "parent_msg_id": "a5646e51-e195-4702-839f-9e7584fa89f3"
            },
            "text/plain": "StatementMeta(Synapsedemo, 0, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(datetime.date(2025, 9, 12), datetime.date(2025, 9, 13))"
          },
          "execution_count": 29,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# setting dates\n",
        "start_date = date.today() - timedelta(1)\n",
        "end_date = date.today()\n",
        "\n",
        "start_date, end_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Synapsedemo",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-09-13T08:38:01.7587634Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-13T08:38:01.7605394Z",
              "execution_finish_time": "2025-09-13T08:38:14.6322328Z",
              "parent_msg_id": "b9452f6c-7b53-4e56-80c5-856d6b41a525"
            },
            "text/plain": "StatementMeta(Synapsedemo, 0, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully saved to abfss://raw-bronze@synapsetest1298.dfs.core.windows.net//2025-09-12_earthquake_data.json\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# Construct API URL from date\n",
        "url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_date}&endtime={end_date}\"\n",
        "\n",
        "try:\n",
        "    # Make the GET request to fetch data\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "    data = response.json().get('features', [])\n",
        "\n",
        "    if not data:\n",
        "        print(\"No data returned for the specified date range.\")\n",
        "    else:\n",
        "        # Specify the ADLS path\n",
        "        file_path = f\"{bronze_adls}/{start_date}_earthquake_data.json\"\n",
        "\n",
        "        # Convert data to JSON string\n",
        "        json_data = json.dumps(data, indent=4)\n",
        "\n",
        "        # Write the JSON data to ADLS\n",
        "        # Create an RDD with the JSON string and parallelize it\n",
        "        rdd = spark.sparkContext.parallelize([json_data])\n",
        "\n",
        "        # Convert RDD to DataFrame and write to ADLS\n",
        "        df = spark.read.json(rdd)\n",
        "        df.limit(100) # To speed up processing\n",
        "        df.write.mode(\"overwrite\").json(file_path)\n",
        "\n",
        "        print(f\"Data successfully saved to {file_path}\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from API: {e}\")\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Synapsedemo",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-09-13T08:40:07.2986033Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-13T08:40:07.3003825Z",
              "execution_finish_time": "2025-09-13T08:40:07.5948472Z",
              "parent_msg_id": "8c39ce44-352e-4daa-b73d-4c50b020377b"
            },
            "text/plain": "StatementMeta(Synapsedemo, 0, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'type': 'Feature',\n 'properties': {'mag': 0.9,\n  'place': '8 km NNW of The Geysers, CA',\n  'time': 1757721597550,\n  'updated': 1757723237978,\n  'tz': None,\n  'url': 'https://earthquake.usgs.gov/earthquakes/eventpage/nc75237097',\n  'detail': 'https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=nc75237097&format=geojson',\n  'felt': None,\n  'cdi': None,\n  'mmi': None,\n  'alert': None,\n  'status': 'automatic',\n  'tsunami': 0,\n  'sig': 12,\n  'net': 'nc',\n  'code': '75237097',\n  'ids': ',nc75237097,',\n  'sources': ',nc,',\n  'types': ',nearby-cities,origin,phase-data,scitech-link,',\n  'nst': 16,\n  'dmin': 0.007739,\n  'rms': 0.02,\n  'gap': 82,\n  'magType': 'md',\n  'type': 'earthquake',\n  'title': 'M 0.9 - 8 km NNW of The Geysers, CA'},\n 'geometry': {'type': 'Point',\n  'coordinates': [-122.817337036133, 38.8286666870117, 1.79999995231628]},\n 'id': 'nc75237097'}"
          },
          "execution_count": 33,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# just print the first item\n",
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Synapsedemo",
              "statement_id": 18,
              "statement_ids": [
                18
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-09-13T08:41:34.0997609Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-13T08:41:34.1016187Z",
              "execution_finish_time": "2025-09-13T08:41:34.4142335Z",
              "parent_msg_id": "59ebaa9b-2d47-423c-8b6b-1103d6921433"
            },
            "text/plain": "StatementMeta(Synapsedemo, 0, 18, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-12_earthquake_data.json True False abfss://raw-bronze@synapsetest1298.dfs.core.windows.net/2025-09-12_earthquake_data.json 0 1757752683341\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# Check if the raw data is in the bronze layer\n",
        "\n",
        "files = mssparkutils.fs.ls(bronze_adls)\n",
        "for file in files:\n",
        "    print(file.name, file.isDir, file.isFile, file.path, file.size, file.modifyTime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "Synapsedemo",
              "statement_id": 19,
              "statement_ids": [
                19
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "normalized_state": "finished",
              "queued_time": "2025-09-13T08:45:36.5167159Z",
              "session_start_time": null,
              "execution_start_time": "2025-09-13T08:45:36.5184546Z",
              "execution_finish_time": "2025-09-13T08:45:36.815886Z",
              "parent_msg_id": "6b12aaa6-84e2-4c9b-8845-3831037539b1"
            },
            "text/plain": "StatementMeta(Synapsedemo, 0, 19, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExitValue: {\"start_date\": \"2025-09-12\", \"bronze_adls\": \"abfss://raw-bronze@synapsetest1298.dfs.core.windows.net/\", \"silver_adls\": \"abfss://refined-silver@synapsetest1298.dfs.core.windows.net/\", \"gold_adls\": \"abfss://business-gold@synapsetest1298.dfs.core.windows.net/\"}"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "# passing metadata/output values from the bronze notebook to the next pipeline step\n",
        "\n",
        "# Define your variables\n",
        "output_data = {\n",
        "    \"start_date\": start_date.isoformat(),\n",
        "    \"bronze_adls\": bronze_adls,\n",
        "    \"silver_adls\": silver_adls,\n",
        "    \"gold_adls\": gold_adls\n",
        "}\n",
        "\n",
        "# Serialize the dictionary to a JSON string\n",
        "bronze_output = json.dumps(output_data)\n",
        "\n",
        "# Use mssparkutils.notebook.exit() to pass the JSON output to the pipeline\n",
        "mssparkutils.notebook.exit(bronze_output)\n",
        ""
      ]
    }
  ]
}